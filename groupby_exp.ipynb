{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,max,avg,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/11 11:41:10 WARN Utils: Your hostname, AI-CJB-LAP-459 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp0s20f3)\n",
      "24/09/11 11:41:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/11 11:41:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/11 11:41:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/11 11:41:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"groupby.com\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age| hike|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  Raman|   Finance|   CA| 99000| 40|24000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns=[\"name\",\"department\",\"state\",\"salary\",\"age\",\"hike\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     257000|\n",
      "|   Finance|     351000|\n",
      "| Marketing|     171000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suming up two columns salary and hike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+--------------+\n",
      "|   name|department|state|salary|age| hike|revised salary|\n",
      "+-------+----------+-----+------+---+-----+--------------+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|        100000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|        106000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|        104000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|        113000|\n",
      "|  Raman|   Finance|   CA| 99000| 40|24000|        123000|\n",
      "|  Scott|   Finance|   NY| 83000| 36|19000|        102000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|         94000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|         98000|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|        112000|\n",
      "+-------+----------+-----+------+---+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_sal_hike=df.withColumn(\"revised salary\",col(\"salary\")+col(\"hike\"))\n",
    "new =sum_sal_hike.groupby('department').agg(max(\"revised salary\").alias(\"revised_salary\"))\n",
    "# new.show()\n",
    "sum_sal_hike.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|state|avg(revised salary)|\n",
      "+-----+-------------------+\n",
      "|   NY|           102800.0|\n",
      "|   CA|           109500.0|\n",
      "+-----+-------------------+\n",
      "\n",
      "+----------+-------------------+\n",
      "|department|avg(revised salary)|\n",
      "+----------+-------------------+\n",
      "|     Sales| 103333.33333333333|\n",
      "|   Finance|           108000.0|\n",
      "| Marketing|           105000.0|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_sal_hike.groupBy(\"state\").avg(\"revised salary\").show()\n",
    "sum_sal_hike.groupBy(\"department\").mean(\"revised salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING GROUP BY WITH MULTIPLE COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+---------+\n",
      "|department|state|sum(salary)|sum(hike)|\n",
      "+----------+-----+-----------+---------+\n",
      "|Sales     |NY   |176000     |30000    |\n",
      "|Sales     |CA   |81000      |23000    |\n",
      "|Finance   |CA   |189000     |47000    |\n",
      "|Finance   |NY   |162000     |34000    |\n",
      "|Marketing |NY   |91000      |21000    |\n",
      "|Marketing |CA   |80000      |18000    |\n",
      "+----------+-----+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\",\"state\")\\\n",
    "    .sum(\"salary\",\"hike\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING MORE AGGRGATE IN ONE TIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+--------+\n",
      "|department|total_salary|max_salary|total_hike|max_hike|\n",
      "+----------+------------+----------+----------+--------+\n",
      "|     Sales|      257000|     90000|     53000|   23000|\n",
      "|   Finance|      351000|     99000|     81000|   24000|\n",
      "| Marketing|      171000|     91000|     39000|   21000|\n",
      "+----------+------------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\") \\\n",
    ".agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    sum(\"hike\").alias(\"total_hike\"),\n",
    "    max(\"hike\").alias(\"max_hike\")\n",
    "    ) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, max\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(\n",
    "      sum(\"salary\").alias(\"sum_salary\"),\n",
    "      avg(\"salary\").alias(\"avg_salary\"),\n",
    "      sum(\"hike\").alias(\"sum_bonus\"),\n",
    "      max(\"hike\").alias(\"max_bonus\")\n",
    "  ) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING WHERE CLAUSE ON AGGREGATE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+--------+\n",
      "|department|total_salary|max_salary|total_hike|max_hike|\n",
      "+----------+------------+----------+----------+--------+\n",
      "|     Sales|      257000|     90000|     53000|   23000|\n",
      "|   Finance|      351000|     99000|     81000|   24000|\n",
      "| Marketing|      171000|     91000|     39000|   21000|\n",
      "+----------+------------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\") \\\n",
    ".agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    sum(\"hike\").alias(\"total_hike\"),\n",
    "    max(\"hike\").alias(\"max_hike\")\n",
    "    ) \\\n",
    "        .where(col(\"total_hike\")>=9000)\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING SQL GROUP BY QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|       avg_salary|sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|     Sales|    257000|85666.66666666667|    53000|    23000|\n",
      "|   Finance|    351000|          87750.0|    81000|    24000|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Using SQL Query\n",
    "sql_string = \"\"\"SELECT department,\n",
    "       SUM(salary) AS sum_salary,\n",
    "       AVG(salary) AS avg_salary,\n",
    "       SUM(hike) AS sum_bonus,\n",
    "       MAX(hike) AS max_bonus\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "HAVING SUM(hike) >= 50000\"\"\"\n",
    "\n",
    "# Execute SQL query against the temporary view\n",
    "df2 = spark.sql(sql_string)\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
