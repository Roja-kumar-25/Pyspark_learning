{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/ai/.local/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/ai/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 14:49:01 WARN Utils: Your hostname, AI-CJB-LAP-459 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp0s20f3)\n",
      "24/09/10 14:49:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/10 14:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|_c0| _c1|   _c2|\n",
      "+---+----+------+\n",
      "| ID|name|salary|\n",
      "|  1|  A1| 30000|\n",
      "|  2|  A2| 10000|\n",
      "|  3|  A3| 20000|\n",
      "|  4|  A4| 15000|\n",
      "+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark= SparkSession.builder.appName(\"csvtootherfiles.com\").getOrCreate()\n",
    "df=spark.read.csv(\"/home/ai/Documents/sampledataset.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|_c0| _c1|   _c2|\n",
      "+---+----+------+\n",
      "| ID|name|salary|\n",
      "|  1|  A1| 30000|\n",
      "|  2|  A2| 10000|\n",
      "|  3|  A3| 20000|\n",
      "|  4|  A4| 15000|\n",
      "|  5|  A5| 25000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING THE CSV FILE TO THE PARQUET FILE :\n",
    "Here the csv file is loaded and stored as dataframe and that dataframe is over written as parquet file and then the parquet file is loaded as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"/home/ai/Documents/sampledataset.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|_c0| _c1|   _c2|\n",
      "+---+----+------+\n",
      "| ID|name|salary|\n",
      "|  1|  A1| 30000|\n",
      "|  2|  A2| 10000|\n",
      "|  3|  A3| 20000|\n",
      "|  4|  A4| 15000|\n",
      "+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"/home/ai/Documents/sampledataset.parquet\")\n",
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING CUSTOM SCHEMA AND STORING IT IN A CSV FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 14:49:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVtoParquet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"RecordNumber\", IntegerType(), True) \\\n",
    "    .add(\"Zipcode\", IntegerType(), True) \\\n",
    "    .add(\"ZipCodeType\", StringType(), True) \\\n",
    "    .add(\"City\", StringType(), True) \\\n",
    "    .add(\"State\", StringType(), True) \\\n",
    "    .add(\"LocationType\", StringType(), True) \\\n",
    "    .add(\"Lat\", DoubleType(), True) \\\n",
    "    .add(\"Long\", DoubleType(), True) \\\n",
    "    .add(\"Xaxis\", IntegerType(), True) \\\n",
    "    .add(\"Yaxis\", DoubleType(), True) \\\n",
    "    .add(\"Zaxis\", DoubleType(), True) \\\n",
    "    .add(\"WorldRegion\", StringType(), True) \\\n",
    "    .add(\"Country\", StringType(), True) \\\n",
    "    .add(\"LocationText\", StringType(), True) \\\n",
    "    .add(\"Location\", StringType(), True) \\\n",
    "    .add(\"Decommisioned\", BooleanType(), True) \\\n",
    "    .add(\"TaxReturnsFiled\", StringType(), True) \\\n",
    "    .add(\"EstimatedPopulation\", IntegerType(), True) \\\n",
    "    .add(\"TotalWages\", IntegerType(), True) \\\n",
    "    .add(\"Notes\", StringType(), True)\n",
    "\n",
    "data = [\n",
    "    (1, 12345, \"STANDARD\", \"SampleCity\", \"ST\", \"Urban\", 37.7749, -122.4194, 100, 200.0, 300.0, \"North America\", \"USA\", \"Sample Location\", \"Sample Text\", False, \"1000\", 50000, 50000, \"No notes\"),\n",
    "    # Add more sample rows as needed\n",
    "]\n",
    "# df = spark.createDataFrame(data, schema)\n",
    "# # Read CSV file into DataFrame with schema\n",
    "# df.write.mode(\"overwrite\").csv(\"/home/ai/Documents/zipcodes.csv\")\n",
    "\n",
    "# df_read = spark.read.format(\"csv\").option(\"header\", True).load(\"/home/ai/Documents/zipcodes.csv\")\n",
    "# df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+----------+---+-----+-------+---------+---+-----+-----+-------------+---+---------------+-----------+-----+----+-------+-------+--------+\n",
      "|  1|12345|STANDARD|SampleCity| ST|Urban|37.7749|-122.4194|100|200.0|300.0|North America|USA|Sample Location|Sample Text|false|1000|5000017|5000018|No notes|\n",
      "+---+-----+--------+----------+---+-----+-------+---------+---+-----+-----+-------------+---+---------------+-----------+-----+----+-------+-------+--------+\n",
      "+---+-----+--------+----------+---+-----+-------+---------+---+-----+-----+-------------+---+---------------+-----------+-----+----+-------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 14:52:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 12345, STANDARD, SampleCity, ST, Urban, 37.7749, -122.4194, 100, 200.0, 300.0, North America, USA, Sample Location, Sample Text, false, 1000, 50000, 50000, No notes\n",
      " Schema: 1, 12345, STANDARD, SampleCity, ST, Urban, 37.7749, -122.4194, 100, 200.0, 300.0, North America, USA, Sample Location, Sample Text, false, 1000, 5000017, 5000018, No notes\n",
      "Expected: 5000017 but found: 50000\n",
      "CSV file: file:///home/ai/Documents/zipcodes.csv/part-00007-539f0e08-cb43-4940-b686-1358e98cc4b6-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# data = [\n",
    "#     (1, 12345, \"STANDARD\", \"SampleCity\", \"ST\", \"Urban\", 37.7749, -122.4194, 100, 200.0, 300.0, \"North America\", \"USA\", \"Sample Location\", \"Sample Text\", False, \"1000\", 50000, 50000, \"No notes\"),\n",
    "#     # Add more sample rows as needed\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, schema)\n",
    "# # Read CSV file into DataFrame with schema\n",
    "# df.write.mode(\"overwrite\").csv(\"/home/ai/Documents/zipcodes.csv\")\n",
    "\n",
    "df_read = spark.read.option(\"header\",\"true\").csv(\"/home/ai/Documents/zipcodes.csv\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
